{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training Job \n",
    "\n",
    "### Please go through this notebook only if you have finished Part 1 to Part 4 of the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 1: Import packages, get IAM role, get the region and set the S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import re\n",
    "import copy\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "bucket ='machinelearning-sagemaker-train' # Put your s3 bucket name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 2: Create the algorithm image and push to Amazon ECR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  5.712MB\r",
      "\r\n",
      "Step 1/10 : FROM python:3.6-buster\n",
      " ---> 6a16f0d68245\n",
      "Step 2/10 : LABEL project=\"keras-sagemaker-train\"\n",
      " ---> Using cache\n",
      " ---> ec513aa7d4df\n",
      "Step 3/10 : ARG APP_HOME=/opt/program\n",
      " ---> Using cache\n",
      " ---> 2a94c9918317\n",
      "Step 4/10 : ENV PATH=\"${APP_HOME}:${PATH}\"\n",
      " ---> Using cache\n",
      " ---> 9d9308191e66\n",
      "Step 5/10 : RUN pip3 install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 2df8438249ce\n",
      "Step 6/10 : RUN pip3 install --upgrade setuptools\n",
      " ---> Using cache\n",
      " ---> 832057067150\n",
      "Step 7/10 : ADD requirements-cpu.txt /\n",
      " ---> Using cache\n",
      " ---> 5e8eed9d27cc\n",
      "Step 8/10 : RUN pip3 install -r requirements-cpu.txt\n",
      " ---> Using cache\n",
      " ---> dc374004dfa1\n",
      "Step 9/10 : COPY src ${APP_HOME}\n",
      " ---> Using cache\n",
      " ---> 79cbf08a84a9\n",
      "Step 10/10 : WORKDIR ${APP_HOME}\n",
      " ---> Using cache\n",
      " ---> 8cd35b869bbd\n",
      "Successfully built 8cd35b869bbd\n",
      "Successfully tagged ml-sagemaker-train:latest\n",
      "The push refers to repository [882096543472.dkr.ecr.us-east-1.amazonaws.com/ml-sagemaker-train]\n",
      "d7472da051e8: Preparing\n",
      "4ad1bd5066a2: Preparing\n",
      "05262154cc21: Preparing\n",
      "381d0b585ca8: Preparing\n",
      "73127be66571: Preparing\n",
      "2d3ee6e6c217: Preparing\n",
      "cd408f7623af: Preparing\n",
      "0b6b0cacd4b2: Preparing\n",
      "bbbb6b6cf09b: Preparing\n",
      "6ca8686315b6: Preparing\n",
      "9c2b7d0c8e89: Preparing\n",
      "79a45871588c: Preparing\n",
      "bdfff16e8653: Preparing\n",
      "dc4f2875405c: Preparing\n",
      "2d3ee6e6c217: Waiting\n",
      "cd408f7623af: Waiting\n",
      "0b6b0cacd4b2: Waiting\n",
      "bbbb6b6cf09b: Waiting\n",
      "6ca8686315b6: Waiting\n",
      "9c2b7d0c8e89: Waiting\n",
      "79a45871588c: Waiting\n",
      "bdfff16e8653: Waiting\n",
      "dc4f2875405c: Waiting\n",
      "381d0b585ca8: Layer already exists\n",
      "d7472da051e8: Layer already exists\n",
      "4ad1bd5066a2: Layer already exists\n",
      "73127be66571: Layer already exists\n",
      "05262154cc21: Layer already exists\n",
      "2d3ee6e6c217: Layer already exists\n",
      "cd408f7623af: Layer already exists\n",
      "6ca8686315b6: Layer already exists\n",
      "bbbb6b6cf09b: Layer already exists\n",
      "0b6b0cacd4b2: Layer already exists\n",
      "79a45871588c: Layer already exists\n",
      "9c2b7d0c8e89: Layer already exists\n",
      "bdfff16e8653: Layer already exists\n",
      "dc4f2875405c: Layer already exists\n",
      "latest: digest: sha256:6524c2341823a24e6ec2c90c6e7225123ddf7a6dcf7017b08cf5886e1e8938d4 size: 3268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Redirecting to /bin/systemctl restart docker.service\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=ml-sagemaker-train\n",
    "\n",
    "chmod +x src/*\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email)\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "# On a SageMaker Notebook Instance, the docker daemon may need to be restarted in order\n",
    "# to detect your network configuration correctly.  (This is a known issue.)\n",
    "if [ -d \"/home/ec2-user/SageMaker\" ]; then\n",
    "  sudo service docker restart\n",
    "fi\n",
    "\n",
    "# Comment the line below to use a GPU\n",
    "docker build  -t ${algorithm_name} -f Dockerfile.cpu .\n",
    "\n",
    "# Uncomment the below line if you wish to run on a GPU\n",
    "#docker build  -t ${algorithm_name} -f Dockerfile.gpu . \n",
    "\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 3: Define variables with data location and output location in S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data location - s3://machinelearning-sagemaker-train/data\n",
      "output location - s3://machinelearning-sagemaker-train/output\n"
     ]
    }
   ],
   "source": [
    "data_location = 's3://{}/data'.format(bucket)\n",
    "print(\"data location - \" + data_location)\n",
    "\n",
    "output_location = 's3://{}/output'.format(bucket)\n",
    "print(\"output location - \" + output_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4: Create a SageMaker session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sage\n",
    "sess = sage.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 5: Define variables for account, region and algorithm image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "882096543472.dkr.ecr.us-east-1.amazonaws.com/ml-sagemaker-train\n"
     ]
    }
   ],
   "source": [
    "account = sess.boto_session.client('sts').get_caller_identity()['Account'] # aws account \n",
    "region = sess.boto_session.region_name # aws server region\n",
    "image = '{}.dkr.ecr.{}.amazonaws.com/ml-sagemaker-train'.format(account, region) # algorithm image path in ECR\n",
    "print(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Define hyperparameters to be passed to your algorithm. \n",
    "In this project we are reading two hyperparameters for training. Use of hyperparameters in optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\"batch_size\":128, \"epochs\":30}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 7: Create the training job using SageMaker Estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "classifier = sage.estimator.Estimator(image_uri=image, \n",
    "                                      role=role,\n",
    "                                      train_instance_count=1, \n",
    "                                      train_instance_type='ml.c5.2xlarge',\n",
    "                                      hyperparameters=hyperparameters,\n",
    "                                      output_path=output_location,\n",
    "                                      sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 8: Run the training job by passing the data location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-17 18:11:56 Starting - Starting the training job...\n",
      "2022-11-17 18:12:21 Starting - Preparing the instances for trainingProfilerReport-1668708716: InProgress\n",
      "......\n",
      "2022-11-17 18:13:21 Downloading - Downloading input data...\n",
      "2022-11-17 18:13:46 Training - Training image download completed. Training in progress..\u001b[34m2022-11-17 18:13:50.094190: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34m2022-11-17 18:13:50.142353: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2999995000 Hz\u001b[0m\n",
      "\u001b[34m2022-11-17 18:13:50.142780: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e300dcd420 executing computations on platform Host. Devices:\u001b[0m\n",
      "\u001b[34m2022-11-17 18:13:50.142803: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\u001b[0m\n",
      "\u001b[34m2022-11-17 18:13:54.655841: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\u001b[0m\n",
      "\u001b[34mUsing TensorFlow backend.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\u001b[0m\n",
      "\u001b[34m[name: \"/device:CPU:0\"\u001b[0m\n",
      "\u001b[34mdevice_type: \"CPU\"\u001b[0m\n",
      "\u001b[34mmemory_limit: 268435456\u001b[0m\n",
      "\u001b[34mlocality {\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mincarnation: 5976770093957701558\u001b[0m\n",
      "\u001b[34m, name: \"/device:XLA_CPU:0\"\u001b[0m\n",
      "\u001b[34mdevice_type: \"XLA_CPU\"\u001b[0m\n",
      "\u001b[34mmemory_limit: 17179869184\u001b[0m\n",
      "\u001b[34mlocality {\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mincarnation: 14215773018846966423\u001b[0m\n",
      "\u001b[34mphysical_device_desc: \"device: XLA_CPU device\"\u001b[0m\n",
      "\u001b[34m]\u001b[0m\n",
      "\u001b[34mScript Status - Starting\u001b[0m\n",
      "\u001b[34mReading hyper parameters\u001b[0m\n",
      "\u001b[34mFinished reading the hyper parameters.\u001b[0m\n",
      "\u001b[34mNumber of data samples:  10000\u001b[0m\n",
      "\u001b[34mNumber of data labels:  10000\u001b[0m\n",
      "\u001b[34mNumber of training samples:  ((8000, 784), (8000, 10))\u001b[0m\n",
      "\u001b[34mNumber of test samples:  ((2000, 784), (2000, 10))\u001b[0m\n",
      "\u001b[34mModel: \"sequential_1\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mdense_1 (Dense)              (None, 512)               401920    \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout_1 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_2 (Dense)              (None, 512)               262656    \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdropout_2 (Dropout)          (None, 512)               0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mdense_3 (Dense)              (None, 10)                5130      \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 669,706\u001b[0m\n",
      "\u001b[34mTrainable params: 669,706\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mTrain on 8000 samples, validate on 2000 samples\u001b[0m\n",
      "\u001b[34mEpoch 1/30\n",
      " 128/8000 [..............................] - ETA: 9s - loss: 2.3025 - accuracy: 0.1094\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 1s - loss: 2.2896 - accuracy: 0.1658\u001b[0m\n",
      "\u001b[34m2304/8000 [=======>......................] - ETA: 0s - loss: 2.2601 - accuracy: 0.2174\u001b[0m\n",
      "\u001b[34m3456/8000 [===========>..................] - ETA: 0s - loss: 2.2146 - accuracy: 0.2844\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 2.1487 - accuracy: 0.3333\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 2.0757 - accuracy: 0.3606\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 2.0016 - accuracy: 0.3903\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 1s 71us/step - loss: 1.9277 - accuracy: 0.4176 - val_loss: 1.3601 - val_accuracy: 0.6065\u001b[0m\n",
      "\u001b[34mEpoch 2/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 1.4207 - accuracy: 0.4922\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 1.3246 - accuracy: 0.6062\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 1.2786 - accuracy: 0.6254\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 1.2332 - accuracy: 0.6406\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 1.2007 - accuracy: 0.6440\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 1.1695 - accuracy: 0.6464\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 1.1350 - accuracy: 0.6578\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 50us/step - loss: 1.1097 - accuracy: 0.6637 - val_loss: 0.8016 - val_accuracy: 0.7980\u001b[0m\n",
      "\u001b[34mEpoch 3/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.9676 - accuracy: 0.6797\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.8758 - accuracy: 0.7083\u001b[0m\n",
      "\u001b[34m2304/8000 [=======>......................] - ETA: 0s - loss: 0.8696 - accuracy: 0.7222\u001b[0m\n",
      "\u001b[34m3456/8000 [===========>..................] - ETA: 0s - loss: 0.8513 - accuracy: 0.7312\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.8382 - accuracy: 0.7383\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.8231 - accuracy: 0.7458\u001b[0m\n",
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 0.8117 - accuracy: 0.7503\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 49us/step - loss: 0.8038 - accuracy: 0.7535 - val_loss: 0.5800 - val_accuracy: 0.8590\u001b[0m\n",
      "\u001b[34mEpoch 4/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.6710 - accuracy: 0.7891\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.6916 - accuracy: 0.7875\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.6867 - accuracy: 0.7915\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.6858 - accuracy: 0.7896\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.6811 - accuracy: 0.7862\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.6732 - accuracy: 0.7882\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.6597 - accuracy: 0.7921\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.6570 - accuracy: 0.7940 - val_loss: 0.4866 - val_accuracy: 0.8845\u001b[0m\n",
      "\u001b[34mEpoch 5/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.5464 - accuracy: 0.8594\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.5858 - accuracy: 0.8203\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.5795 - accuracy: 0.8183\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.5803 - accuracy: 0.8198\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.5732 - accuracy: 0.8236\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.5705 - accuracy: 0.8245\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 0s - loss: 0.5603 - accuracy: 0.8288\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.5573 - accuracy: 0.8296 - val_loss: 0.4095 - val_accuracy: 0.8900\u001b[0m\n",
      "\u001b[34mEpoch 6/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.5454 - accuracy: 0.8047\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.5205 - accuracy: 0.8422\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.5083 - accuracy: 0.8433\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.4965 - accuracy: 0.8489\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.5001 - accuracy: 0.8468\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.4966 - accuracy: 0.8492\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 0s - loss: 0.4891 - accuracy: 0.8537\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.4920 - accuracy: 0.8522 - val_loss: 0.3681 - val_accuracy: 0.8995\u001b[0m\n",
      "\u001b[34mEpoch 7/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3587 - accuracy: 0.8594\u001b[0m\n",
      "\u001b[34m1408/8000 [====>.........................] - ETA: 0s - loss: 0.4765 - accuracy: 0.8466\u001b[0m\n",
      "\u001b[34m2560/8000 [========>.....................] - ETA: 0s - loss: 0.4687 - accuracy: 0.8504\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.4623 - accuracy: 0.8561\u001b[0m\n",
      "\u001b[34m4992/8000 [=================>............] - ETA: 0s - loss: 0.4538 - accuracy: 0.8596\u001b[0m\n",
      "\u001b[34m6144/8000 [======================>.......] - ETA: 0s - loss: 0.4504 - accuracy: 0.8615\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.4518 - accuracy: 0.8610\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 47us/step - loss: 0.4528 - accuracy: 0.8608 - val_loss: 0.3042 - val_accuracy: 0.9285\u001b[0m\n",
      "\u001b[34mEpoch 8/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3829 - accuracy: 0.8672\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.3898 - accuracy: 0.8813\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.4176 - accuracy: 0.8721\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.4159 - accuracy: 0.8736\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.4107 - accuracy: 0.8742\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.4117 - accuracy: 0.8745\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 0s - loss: 0.4120 - accuracy: 0.8730\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.4132 - accuracy: 0.8733 - val_loss: 0.2850 - val_accuracy: 0.9205\u001b[0m\n",
      "\u001b[34mEpoch 9/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3921 - accuracy: 0.8672\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.3875 - accuracy: 0.8719\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.4037 - accuracy: 0.8717\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.4040 - accuracy: 0.8747\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.3895 - accuracy: 0.8806\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.3865 - accuracy: 0.8830\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 0s - loss: 0.3852 - accuracy: 0.8831\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.3838 - accuracy: 0.8841 - val_loss: 0.2746 - val_accuracy: 0.9325\u001b[0m\n",
      "\u001b[34mEpoch 10/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.4482 - accuracy: 0.8672\u001b[0m\n",
      "\u001b[34m1408/8000 [====>.........................] - ETA: 0s - loss: 0.3476 - accuracy: 0.8871\u001b[0m\n",
      "\u001b[34m2560/8000 [========>.....................] - ETA: 0s - loss: 0.3568 - accuracy: 0.8848\u001b[0m\n",
      "\u001b[34m3840/8000 [=============>................] - ETA: 0s - loss: 0.3613 - accuracy: 0.8880\u001b[0m\n",
      "\u001b[34m4992/8000 [=================>............] - ETA: 0s - loss: 0.3633 - accuracy: 0.8886\u001b[0m\n",
      "\u001b[34m6144/8000 [======================>.......] - ETA: 0s - loss: 0.3664 - accuracy: 0.8874\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.3639 - accuracy: 0.8891\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 47us/step - loss: 0.3614 - accuracy: 0.8896 - val_loss: 0.2705 - val_accuracy: 0.9230\u001b[0m\n",
      "\u001b[34mEpoch 11/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3034 - accuracy: 0.8984\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.3363 - accuracy: 0.8914\u001b[0m\n",
      "\u001b[34m2560/8000 [========>.....................] - ETA: 0s - loss: 0.3498 - accuracy: 0.8863\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.3511 - accuracy: 0.8914\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.3560 - accuracy: 0.8908\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.3470 - accuracy: 0.8935\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 0s - loss: 0.3421 - accuracy: 0.8961\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.3407 - accuracy: 0.8961 - val_loss: 0.2574 - val_accuracy: 0.9245\u001b[0m\n",
      "\u001b[34mEpoch 12/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3408 - accuracy: 0.8750\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.3234 - accuracy: 0.8938\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.3390 - accuracy: 0.8935\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.3375 - accuracy: 0.8959\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.3261 - accuracy: 0.8995\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.3250 - accuracy: 0.9010\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.3211 - accuracy: 0.9024\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 47us/step - loss: 0.3244 - accuracy: 0.9016 - val_loss: 0.2393 - val_accuracy: 0.9320\u001b[0m\n",
      "\u001b[34mEpoch 13/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2530 - accuracy: 0.9062\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.3223 - accuracy: 0.8961\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.3198 - accuracy: 0.8988\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.3250 - accuracy: 0.9001\u001b[0m\n",
      "\u001b[34m4992/8000 [=================>............] - ETA: 0s - loss: 0.3229 - accuracy: 0.9012\u001b[0m\n",
      "\u001b[34m6144/8000 [======================>.......] - ETA: 0s - loss: 0.3213 - accuracy: 0.9006\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.3111 - accuracy: 0.9036\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 47us/step - loss: 0.3070 - accuracy: 0.9055 - val_loss: 0.2277 - val_accuracy: 0.9380\u001b[0m\n",
      "\u001b[34mEpoch 14/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3024 - accuracy: 0.8828\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2917 - accuracy: 0.9055\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2980 - accuracy: 0.9112\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.3038 - accuracy: 0.9071\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.3053 - accuracy: 0.9082\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.3025 - accuracy: 0.9081\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.2958 - accuracy: 0.9085\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.2941 - accuracy: 0.9086 - val_loss: 0.2207 - val_accuracy: 0.9345\u001b[0m\n",
      "\u001b[34mEpoch 15/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3323 - accuracy: 0.9141\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2389 - accuracy: 0.9180\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2616 - accuracy: 0.9137\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.2639 - accuracy: 0.9174\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2673 - accuracy: 0.9143\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2769 - accuracy: 0.9125\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.2752 - accuracy: 0.9141\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.2824 - accuracy: 0.9126 - val_loss: 0.2238 - val_accuracy: 0.9380\u001b[0m\n",
      "\u001b[34mEpoch 16/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3549 - accuracy: 0.8750\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2781 - accuracy: 0.9180\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2701 - accuracy: 0.9194\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.2567 - accuracy: 0.9210\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2691 - accuracy: 0.9179\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.2630 - accuracy: 0.9195\u001b[0m\n",
      "\u001b[34m7168/8000 [=========================>....] - ETA: 0s - loss: 0.2686 - accuracy: 0.9176\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.2663 - accuracy: 0.9189 - val_loss: 0.2049 - val_accuracy: 0.9430\u001b[0m\n",
      "\u001b[34mEpoch 17/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1969 - accuracy: 0.9375\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2585 - accuracy: 0.9281\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2602 - accuracy: 0.9239\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.2622 - accuracy: 0.9230\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.2518 - accuracy: 0.9274\u001b[0m\n",
      "\u001b[34m6016/8000 [=====================>........] - ETA: 0s - loss: 0.2534 - accuracy: 0.9259\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.2539 - accuracy: 0.9241\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.2560 - accuracy: 0.9234 - val_loss: 0.2115 - val_accuracy: 0.9395\u001b[0m\n",
      "\u001b[34mEpoch 18/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2217 - accuracy: 0.9531\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2199 - accuracy: 0.9367\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2272 - accuracy: 0.9317\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.2344 - accuracy: 0.9294\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2448 - accuracy: 0.9278\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2470 - accuracy: 0.9273\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.2490 - accuracy: 0.9270\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.2494 - accuracy: 0.9265 - val_loss: 0.1932 - val_accuracy: 0.9425\u001b[0m\n",
      "\u001b[34mEpoch 19/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2189 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2055 - accuracy: 0.9367\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2206 - accuracy: 0.9322\u001b[0m\n",
      "\u001b[34m3456/8000 [===========>..................] - ETA: 0s - loss: 0.2215 - accuracy: 0.9314\u001b[0m\n",
      "\u001b[34m4480/8000 [===============>..............] - ETA: 0s - loss: 0.2282 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m5504/8000 [===================>..........] - ETA: 0s - loss: 0.2306 - accuracy: 0.9290\u001b[0m\n",
      "\u001b[34m6528/8000 [=======================>......] - ETA: 0s - loss: 0.2352 - accuracy: 0.9280\u001b[0m\n",
      "\u001b[34m7680/8000 [===========================>..] - ETA: 0s - loss: 0.2345 - accuracy: 0.9284\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 53us/step - loss: 0.2348 - accuracy: 0.9284 - val_loss: 0.1964 - val_accuracy: 0.9410\u001b[0m\n",
      "\u001b[34mEpoch 20/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2756 - accuracy: 0.8984\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2434 - accuracy: 0.9180\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2195 - accuracy: 0.9301\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.2203 - accuracy: 0.9300\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.2196 - accuracy: 0.9333\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.2165 - accuracy: 0.9341\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.2265 - accuracy: 0.9317\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 49us/step - loss: 0.2246 - accuracy: 0.9314 - val_loss: 0.1911 - val_accuracy: 0.9470\u001b[0m\n",
      "\u001b[34mEpoch 21/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.3380 - accuracy: 0.8984\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.2632 - accuracy: 0.9164\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.2434 - accuracy: 0.9260\u001b[0m\n",
      "\u001b[34m3456/8000 [===========>..................] - ETA: 0s - loss: 0.2334 - accuracy: 0.9274\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.2236 - accuracy: 0.9312\u001b[0m\n",
      "\u001b[34m5760/8000 [====================>.........] - ETA: 0s - loss: 0.2166 - accuracy: 0.9342\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m6912/8000 [========================>.....] - ETA: 0s - loss: 0.2138 - accuracy: 0.9332\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 51us/step - loss: 0.2170 - accuracy: 0.9326 - val_loss: 0.1774 - val_accuracy: 0.9520\u001b[0m\n",
      "\u001b[34mEpoch 22/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1500 - accuracy: 0.9531\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.1849 - accuracy: 0.9469\u001b[0m\n",
      "\u001b[34m2304/8000 [=======>......................] - ETA: 0s - loss: 0.1911 - accuracy: 0.9418\u001b[0m\n",
      "\u001b[34m3456/8000 [===========>..................] - ETA: 0s - loss: 0.2024 - accuracy: 0.9407\u001b[0m\n",
      "\u001b[34m4608/8000 [================>.............] - ETA: 0s - loss: 0.1953 - accuracy: 0.9425\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.2040 - accuracy: 0.9396\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.2016 - accuracy: 0.9399\u001b[0m\n",
      "\u001b[34m7936/8000 [============================>.] - ETA: 0s - loss: 0.2043 - accuracy: 0.9386\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 51us/step - loss: 0.2061 - accuracy: 0.9379 - val_loss: 0.2020 - val_accuracy: 0.9395\u001b[0m\n",
      "\u001b[34mEpoch 23/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2929 - accuracy: 0.8906\u001b[0m\n",
      "\u001b[34m1408/8000 [====>.........................] - ETA: 0s - loss: 0.2074 - accuracy: 0.9389\u001b[0m\n",
      "\u001b[34m2560/8000 [========>.....................] - ETA: 0s - loss: 0.1886 - accuracy: 0.9449\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.2078 - accuracy: 0.9378\u001b[0m\n",
      "\u001b[34m4864/8000 [=================>............] - ETA: 0s - loss: 0.2010 - accuracy: 0.9387\u001b[0m\n",
      "\u001b[34m6144/8000 [======================>.......] - ETA: 0s - loss: 0.1992 - accuracy: 0.9408\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1986 - accuracy: 0.9416\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 47us/step - loss: 0.1978 - accuracy: 0.9416 - val_loss: 0.1968 - val_accuracy: 0.9375\u001b[0m\n",
      "\u001b[34mEpoch 24/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2589 - accuracy: 0.9219\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1877 - accuracy: 0.9418\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.2005 - accuracy: 0.9416\u001b[0m\n",
      "\u001b[34m3200/8000 [===========>..................] - ETA: 0s - loss: 0.2012 - accuracy: 0.9425\u001b[0m\n",
      "\u001b[34m4224/8000 [==============>...............] - ETA: 0s - loss: 0.1933 - accuracy: 0.9446\u001b[0m\n",
      "\u001b[34m5248/8000 [==================>...........] - ETA: 0s - loss: 0.1945 - accuracy: 0.9430\u001b[0m\n",
      "\u001b[34m6272/8000 [======================>.......] - ETA: 0s - loss: 0.1884 - accuracy: 0.9440\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1887 - accuracy: 0.9430\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 55us/step - loss: 0.1882 - accuracy: 0.9431 - val_loss: 0.1694 - val_accuracy: 0.9510\u001b[0m\n",
      "\u001b[34mEpoch 25/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2965 - accuracy: 0.9141\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1953 - accuracy: 0.9427\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1971 - accuracy: 0.9412\u001b[0m\n",
      "\u001b[34m3200/8000 [===========>..................] - ETA: 0s - loss: 0.1951 - accuracy: 0.9422\u001b[0m\n",
      "\u001b[34m4352/8000 [===============>..............] - ETA: 0s - loss: 0.1985 - accuracy: 0.9403\u001b[0m\n",
      "\u001b[34m5376/8000 [===================>..........] - ETA: 0s - loss: 0.1963 - accuracy: 0.9410\u001b[0m\n",
      "\u001b[34m6400/8000 [=======================>......] - ETA: 0s - loss: 0.1882 - accuracy: 0.9444\u001b[0m\n",
      "\u001b[34m7424/8000 [==========================>...] - ETA: 0s - loss: 0.1847 - accuracy: 0.9457\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 53us/step - loss: 0.1807 - accuracy: 0.9469 - val_loss: 0.1772 - val_accuracy: 0.9480\u001b[0m\n",
      "\u001b[34mEpoch 26/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2489 - accuracy: 0.9297\u001b[0m\n",
      "\u001b[34m1152/8000 [===>..........................] - ETA: 0s - loss: 0.1885 - accuracy: 0.9418\u001b[0m\n",
      "\u001b[34m2176/8000 [=======>......................] - ETA: 0s - loss: 0.1704 - accuracy: 0.9467\u001b[0m\n",
      "\u001b[34m3328/8000 [===========>..................] - ETA: 0s - loss: 0.1761 - accuracy: 0.9441\u001b[0m\n",
      "\u001b[34m4480/8000 [===============>..............] - ETA: 0s - loss: 0.1751 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m5632/8000 [====================>.........] - ETA: 0s - loss: 0.1751 - accuracy: 0.9457\u001b[0m\n",
      "\u001b[34m6784/8000 [========================>.....] - ETA: 0s - loss: 0.1748 - accuracy: 0.9456\u001b[0m\n",
      "\u001b[34m7936/8000 [============================>.] - ETA: 0s - loss: 0.1744 - accuracy: 0.9452\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 51us/step - loss: 0.1733 - accuracy: 0.9456 - val_loss: 0.1589 - val_accuracy: 0.9540\u001b[0m\n",
      "\u001b[34mEpoch 27/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1041 - accuracy: 0.9688\u001b[0m\n",
      "\u001b[34m1408/8000 [====>.........................] - ETA: 0s - loss: 0.1313 - accuracy: 0.9624\u001b[0m\n",
      "\u001b[34m2560/8000 [========>.....................] - ETA: 0s - loss: 0.1475 - accuracy: 0.9570\u001b[0m\n",
      "\u001b[34m3712/8000 [============>.................] - ETA: 0s - loss: 0.1578 - accuracy: 0.9510\u001b[0m\n",
      "\u001b[34m4992/8000 [=================>............] - ETA: 0s - loss: 0.1616 - accuracy: 0.9499\u001b[0m\n",
      "\u001b[34m6144/8000 [======================>.......] - ETA: 0s - loss: 0.1630 - accuracy: 0.9502\u001b[0m\n",
      "\u001b[34m7296/8000 [==========================>...] - ETA: 0s - loss: 0.1669 - accuracy: 0.9494\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.1693 - accuracy: 0.9486 - val_loss: 0.1742 - val_accuracy: 0.9530\u001b[0m\n",
      "\u001b[34mEpoch 28/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1643 - accuracy: 0.9375\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.1491 - accuracy: 0.9547\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.1466 - accuracy: 0.9589\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.1478 - accuracy: 0.9542\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.1549 - accuracy: 0.9525\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1534 - accuracy: 0.9523\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.1578 - accuracy: 0.9517\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.1581 - accuracy: 0.9517 - val_loss: 0.1551 - val_accuracy: 0.9565\u001b[0m\n",
      "\u001b[34mEpoch 29/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.1296 - accuracy: 0.9531\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.1601 - accuracy: 0.9484\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.1618 - accuracy: 0.9502\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.1559 - accuracy: 0.9526\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.1607 - accuracy: 0.9506\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1579 - accuracy: 0.9509\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.1527 - accuracy: 0.9530\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.1530 - accuracy: 0.9530 - val_loss: 0.1561 - val_accuracy: 0.9580\u001b[0m\n",
      "\u001b[34mEpoch 30/30\n",
      " 128/8000 [..............................] - ETA: 0s - loss: 0.2042 - accuracy: 0.9453\u001b[0m\n",
      "\u001b[34m1280/8000 [===>..........................] - ETA: 0s - loss: 0.1347 - accuracy: 0.9617\u001b[0m\n",
      "\u001b[34m2432/8000 [========>.....................] - ETA: 0s - loss: 0.1488 - accuracy: 0.9560\u001b[0m\n",
      "\u001b[34m3584/8000 [============>.................] - ETA: 0s - loss: 0.1515 - accuracy: 0.9554\u001b[0m\n",
      "\u001b[34m4736/8000 [================>.............] - ETA: 0s - loss: 0.1479 - accuracy: 0.9571\u001b[0m\n",
      "\u001b[34m5888/8000 [=====================>........] - ETA: 0s - loss: 0.1477 - accuracy: 0.9570\u001b[0m\n",
      "\u001b[34m7040/8000 [=========================>....] - ETA: 0s - loss: 0.1454 - accuracy: 0.9578\u001b[0m\n",
      "\u001b[34m8000/8000 [==============================] - 0s 48us/step - loss: 0.1458 - accuracy: 0.9569 - val_loss: 0.1497 - val_accuracy: 0.9590\u001b[0m\n",
      "\u001b[34mTest loss: 0.14972809478640556\u001b[0m\n",
      "\u001b[34mTest accuracy: 0.9589999914169312\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mFinished saving the model.\u001b[0m\n",
      "\u001b[34mFinished training the model.\u001b[0m\n",
      "\u001b[34mScript Status - Finished\u001b[0m\n",
      "\u001b[34mTotal time taken:  16.621195793151855\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2022-11-17 18:14:22 Uploading - Uploading generated training model\n",
      "2022-11-17 18:14:22 Completed - Training job completed\n",
      "Training seconds: 68\n",
      "Billable seconds: 68\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
